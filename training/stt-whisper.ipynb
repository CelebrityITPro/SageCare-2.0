{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Text with Whisper Transfer Learning\n",
    "\n",
    "**Objective:** Fine-tune a Whisper base model on the United-Syn-Med dataset to improve medical speech transcription accuracy in a live teleconsultation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T00:47:44.008168Z",
     "iopub.status.busy": "2025-06-02T00:47:44.007869Z",
     "iopub.status.idle": "2025-06-02T00:49:13.542751Z",
     "shell.execute_reply": "2025-06-02T00:49:13.542023Z",
     "shell.execute_reply.started": "2025-06-02T00:47:44.008144Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\ifedi\\appdata\\local\\temp\\pip-req-build-y7g0svdv\n",
      "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.3/65.3 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting numba\n",
      "  Downloading numba-0.61.2-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from openai-whisper==20240930) (1.24.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "     -------------------------------------- 893.9/893.9 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from openai-whisper==20240930) (2.0.0+cpu)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl (30.3 MB)\n",
      "     ---------------------------------------- 30.3/30.3 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from tiktoken->openai-whisper==20240930) (2.32.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from tqdm->openai-whisper==20240930) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=813054 sha256=3fb08166fd2366db81fc14b0831a0a98ed37dda3803ea1ec707e7dddd7d59cdf\n",
      "  Stored in directory: C:\\Users\\ifedi\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x3cyp8_9\\wheels\\1f\\1d\\98\\9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: tqdm, regex, more-itertools, llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.44.0 more-itertools-10.7.0 numba-0.61.2 openai-whisper-20240930 regex-2024.11.6 tiktoken-0.9.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\ifedi\\AppData\\Local\\Temp\\pip-req-build-y7g0svdv'\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "     -------------------------------------- 491.5/491.5 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "     ---------------------------------------- 10.5/10.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "     -------------------------------------- 362.1/362.1 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting click>=8.1.8\n",
      "  Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Collecting rapidfuzz>=3.9.7\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "     ---------------------------------------- 25.8/25.8 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 11.1/11.1 MB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "     -------------------------------------- 143.5/143.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "     -------------------------------------- 514.8/514.8 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Collecting torch==2.7.1\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "     -------------------------------------- 216.1/216.1 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (3.1.6)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "     -------------------------------------- 181.4/181.4 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "     -------------------------------------- 117.6/117.6 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.12-cp311-cp311-win_amd64.whl (451 kB)\n",
      "     -------------------------------------- 451.4/451.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.4-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 86.7/86.7 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from jinja2->torch==2.7.1->torchaudio) (3.0.2)\n",
      "Installing collected packages: pytz, xxhash, tzdata, safetensors, rapidfuzz, pyyaml, pycparser, pyarrow, propcache, multidict, fsspec, frozenlist, dill, click, aiohappyeyeballs, yarl, torch, pandas, multiprocess, jiwer, huggingface-hub, cffi, aiosignal, torchaudio, tokenizers, soundfile, aiohttp, accelerate, transformers, datasets\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0+cpu\n",
      "    Uninstalling torch-2.0.0+cpu:\n",
      "      Successfully uninstalled torch-2.0.0+cpu\n",
      "Successfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.12 aiosignal-1.3.2 cffi-1.17.1 click-8.2.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 huggingface-hub-0.33.0 jiwer-3.1.0 multidict-6.4.4 multiprocess-0.70.16 pandas-2.3.0 propcache-0.3.2 pyarrow-20.0.0 pycparser-2.22 pytz-2025.2 pyyaml-6.0.2 rapidfuzz-3.13.0 safetensors-0.5.3 soundfile-0.13.1 tokenizers-0.21.1 torch-2.7.1 torchaudio-2.7.1 transformers-4.52.4 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.1+cpu requires torch==2.0.0, but you have torch 2.7.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install jiwer datasets torchaudio transformers accelerate soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:08:21.803995Z",
     "iopub.status.busy": "2025-06-02T02:08:21.803729Z",
     "iopub.status.idle": "2025-06-02T02:08:21.808381Z",
     "shell.execute_reply": "2025-06-02T02:08:21.807585Z",
     "shell.execute_reply.started": "2025-06-02T02:08:21.803978Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# import dependent libraries\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from datasets import Dataset, DatasetDict\n",
    "from jiwer import wer, cer\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torchaudio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T01:40:12.529640Z",
     "iopub.status.busy": "2025-06-02T01:40:12.528920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data-collection/UnitedSynMed.rar\n",
      "../data-collection/UnitedSynMed\\audio\\test\\drug-brand-en-us-female-0031c803-9529-4e8c-85cc-e69baedc152c.mp3\n",
      "../data-collection/UnitedSynMed\\audio\\test\\drug-brand-en-us-female-00c6b893-719c-4550-8907-55177ce89e99.mp3\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "import os\n",
    "n = 0\n",
    "for dirname, _, filenames in os.walk('../data-collection/'):\n",
    "    for filename in filenames:\n",
    "        if n < 3:\n",
    "            print(os.path.join(dirname, filename))\n",
    "            n += 1\n",
    "        else: break\n",
    "    if n >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:08:26.954263Z",
     "iopub.status.busy": "2025-06-02T02:08:26.953453Z",
     "iopub.status.idle": "2025-06-02T02:08:31.600892Z",
     "shell.execute_reply": "2025-06-02T02:08:31.600323Z",
     "shell.execute_reply.started": "2025-06-02T02:08:26.954230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths to the dataset\n",
    "audio_root = \"../data-collection/UnitedSynMed/audio\"\n",
    "transcript_root = \"../data-collection/UnitedSynMed/transcript/\"\n",
    "\n",
    "# Load CSVs and match them with audio paths\n",
    "def load_split(split):\n",
    "    csv_path = os.path.join(transcript_root, f\"{split}.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"path\"] = df[\"file_name\"].apply(lambda x: os.path.join(audio_root, split, x))\n",
    "    return df\n",
    "\n",
    "# Create datasets\n",
    "train_df = load_split(\"train\")\n",
    "test_df = load_split(\"test\")\n",
    "val_df = load_split(\"validation\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\": Dataset.from_pandas(test_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:08:58.456975Z",
     "iopub.status.busy": "2025-06-02T02:08:58.456312Z",
     "iopub.status.idle": "2025-06-02T02:08:58.464123Z",
     "shell.execute_reply": "2025-06-02T02:08:58.463337Z",
     "shell.execute_reply.started": "2025-06-02T02:08:58.456951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': ['drug-female-ca56662a-065c-4318-b1c6-41095684a20a.mp3',\n",
       "  'drug-male-4e02e7a8-ee16-4943-9b7f-4e09d32b558a.mp3',\n",
       "  'drug-male-133fccfc-22ea-4b3b-a89c-9b5df813bff8.mp3',\n",
       "  'drug-female-d0565c6e-c35b-43a0-b55b-624802e078c1.mp3',\n",
       "  'drug-brand-en-us-female-9c816919-b532-4ffb-8be7-08c4a0a671fc.mp3'],\n",
       " 'transcription': ['Meglumine diatrizoate is commonly used as a contrast medium in medical imaging procedures.',\n",
       "  'ZOTACAL is a prescription medication used to treat osteoporosis by increasing bone density.',\n",
       "  'The extract from pongamia pinnata seeds is gaining popularity for its potential therapeutic properties.',\n",
       "  'The use of aminoacridine may offer a new avenue for the treatment of certain types of leukemia.',\n",
       "  'Bolofen is commonly used as a muscle relaxant medication.'],\n",
       " 'path': ['../data-collection/UnitedSynMed/audio\\\\train\\\\drug-female-ca56662a-065c-4318-b1c6-41095684a20a.mp3',\n",
       "  '../data-collection/UnitedSynMed/audio\\\\train\\\\drug-male-4e02e7a8-ee16-4943-9b7f-4e09d32b558a.mp3',\n",
       "  '../data-collection/UnitedSynMed/audio\\\\train\\\\drug-male-133fccfc-22ea-4b3b-a89c-9b5df813bff8.mp3',\n",
       "  '../data-collection/UnitedSynMed/audio\\\\train\\\\drug-female-d0565c6e-c35b-43a0-b55b-624802e078c1.mp3',\n",
       "  '../data-collection/UnitedSynMed/audio\\\\train\\\\drug-brand-en-us-female-9c816919-b532-4ffb-8be7-08c4a0a671fc.mp3']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:18:23.823683Z",
     "iopub.status.busy": "2025-06-02T02:18:23.823109Z",
     "iopub.status.idle": "2025-06-02T02:18:23.860457Z",
     "shell.execute_reply": "2025-06-02T02:18:23.859531Z",
     "shell.execute_reply.started": "2025-06-02T02:18:23.823661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All audio resampled and saved to: C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/UnitedSynMed/audio_resampled\n"
     ]
    }
   ],
   "source": [
    "# Define source and target folders\n",
    "source_root = \"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/UnitedSynMed/audio\"\n",
    "target_root = \"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/UnitedSynMed/audio_resampled\"\n",
    "target_sample_rate = 16000\n",
    "\n",
    "os.makedirs(target_root, exist_ok=True)\n",
    "\n",
    "splits = ['train', 'test', 'validation']\n",
    "\n",
    "for split in splits:\n",
    "    src_dir = os.path.join(source_root, split)\n",
    "    tgt_dir = os.path.join(target_root, split)\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "\n",
    "    audio_files = glob.glob(os.path.join(src_dir, \"*.mp3\"))\n",
    "\n",
    "    for file in audio_files:\n",
    "        waveform, sr = torchaudio.load(file)\n",
    "        if sr != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        filename = os.path.splitext(os.path.basename(file))[0] + \".wav\"\n",
    "        torchaudio.save(os.path.join(tgt_dir, filename), waveform, target_sample_rate)\n",
    "\n",
    "print(\"✅ All audio resampled and saved to:\", target_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paths to the dataset\n",
    "# audio_root = \"../data-collection/UnitedSynMed/audio_resampled\"\n",
    "# transcript_root = \"../data-collection/UnitedSynMed/transcript/\"\n",
    "\n",
    "# # Load CSVs and match them with audio paths\n",
    "# def load_split(split):\n",
    "#     csv_path = os.path.join(transcript_root, f\"{split}.csv\")\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     df[\"path\"] = df[\"file_name\"].apply(lambda x: os.path.join(audio_root, split, x))\n",
    "#     return df\n",
    "\n",
    "# # Create datasets\n",
    "# train_df = load_split(\"train\")\n",
    "# test_df = load_split(\"test\")\n",
    "# val_df = load_split(\"validation\")\n",
    "\n",
    "# # Convert to Hugging Face Dataset\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": Dataset.from_pandas(train_df),\n",
    "#     \"test\": Dataset.from_pandas(test_df),\n",
    "#     \"validation\": Dataset.from_pandas(val_df)\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the dataset\n",
    "audio_root = \"../data-collection/UnitedSynMed/audio_resampled\"\n",
    "transcript_root = \"../data-collection/UnitedSynMed/transcript/\"\n",
    "\n",
    "# Load CSVs and match them with audio paths\n",
    "def load_split(split):\n",
    "    csv_path = os.path.join(transcript_root, f\"{split}.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"path\"] = df[\"file_name\"].apply(lambda x: os.path.join(audio_root, split, x[:-4] + \".wav\"))\n",
    "    return df\n",
    "\n",
    "# Create datasets\n",
    "train_df = load_split(\"train\")\n",
    "test_df = load_split(\"test\")\n",
    "val_df = load_split(\"validation\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\": Dataset.from_pandas(test_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-02T02:28:57.003Z",
     "iopub.execute_input": "2025-06-02T02:19:38.165845Z",
     "iopub.status.busy": "2025-06-02T02:19:38.165539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  57%|█████▋    | 35999/63250 [45:59<34:49, 13.04 examples/s]     \n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 68719476736 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3517\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3517\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3518\u001b[39m num_examples_progress_update += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:552\u001b[39m, in \u001b[36mArrowWriter.write\u001b[39m\u001b[34m(self, example, key, writer_batch_size)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:630\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    629\u001b[39m pa_table = pa.Table.from_arrays(arrays, schema=schema)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:648\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28mself\u001b[39m._num_examples += pa_table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpa_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\pyarrow\\ipc.pxi:529\u001b[39m, in \u001b[36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowMemoryError\u001b[39m: realloc of size 68719476736 failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mArrowMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\dataset_dict.py:944\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    942\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    966\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3552\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[32m   3551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3552\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3554\u001b[39m         tmp_file.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:657\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# Re-initializing to empty list for next batch\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:630\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    628\u001b[39m schema = inferred_features.arrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema\n\u001b[32m    629\u001b[39m pa_table = pa.Table.from_arrays(arrays, schema=schema)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\datasets\\arrow_writer.py:648\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28mself\u001b[39m._num_bytes += pa_table.nbytes\n\u001b[32m    647\u001b[39m \u001b[38;5;28mself\u001b[39m._num_examples += pa_table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpa_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\pyarrow\\ipc.pxi:529\u001b[39m, in \u001b[36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowMemoryError\u001b[39m: realloc of size 68719476736 failed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Whisper processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Set target sample rate\n",
    "target_sample_rate = 16000\n",
    "\n",
    "def preprocess(batch):\n",
    "    audio_input, sr = sf.read(batch[\"path\"])\n",
    "    \n",
    "    # If the sample rate is not 16kHz, resample it\n",
    "    if sr != target_sample_rate:\n",
    "        waveform = torch.tensor(audio_input).float()\n",
    "        if len(waveform.shape) > 1 and waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0)  # Convert to mono\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sample_rate)\n",
    "        audio_input = resampler(waveform).numpy()\n",
    "    \n",
    "    inputs = processor(audio_input, sampling_rate=target_sample_rate, return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "# Freeze encoder layers\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-medical\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    cer_score = cer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer_score, \"cer\": cer_score}\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"whisper-medical-finetuned\")\n",
    "processor.save_pretrained(\"whisper-medical-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7569174,
     "sourceId": 12030144,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "my_pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
